<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="theme-color" content="#000000" />
    <link rel="stylesheet" href="reset.css" />
    <link rel="stylesheet" href="index.css" />
    <link rel="stylesheet" href="public/prism.css" />
    <script src="public/prism.js"></script>
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/cytoscape/3.28.1/cytoscape.min.js"
      integrity="sha512-RcuA+PEnJcg1caTn53YLhZ3bYVFXphzcPL1BjBoAwFiA3bErav+AndZz1xrqpAtv/8Waep2X+9zn8KWpwacUSA=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>
    <!-- <script
      src="https://cdnjs.cloudflare.com/ajax/libs/cytoscape/3.28.1/cytoscape.umd.js"
      integrity="sha512-LxtJh1JTtBgZQaf0DF+4f/thp9An8o8UC35/YaDTYtHIOCPIuEDZRYRCLDw++hEvm/AFlGOM5HmgAMuSkxzsyg=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
      ></script> -->
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/dagre/0.8.5/dagre.min.js"
      integrity="sha512-psLUZfcgPmi012lcpVHkWoOqyztollwCGu4w/mXijFMK/YcdUdP06voJNVOJ7f/dUIlO2tGlDLuypRyXX2lcvQ=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>
    <script src="cytoscape-dagre.js"></script>
    <title>LLM code gen</title>
  </head>
  <body>
    <h1>Monte Carlo Tree Search for Code Generation using LLMs</h1>
    <p>
      There have been a slew of LLMs released to the public over the past ~12 months that have shown
      surprising levels of knowledge & ability in multiple domains. Code generation in particular
      has received quite a lot of attention, with a continuous stream of new models being published
      almost monthly–notable examples including OpenAI's
      <a href="https://openai.com/gpt-4" target="_blank">GPT-4</a> &
      <a href="https://platform.openai.com/docs/models/gpt-3-5" target="_blank">GPT-3.5</a>,
      Microsoft's
      <a
        href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/"
        target="_blank"
        >Phi-2</a
      >, Meta's
      <a href="https://ai.meta.com/blog/code-llama-large-language-model-coding/" target="_blank"
        >CodeLlama</a
      >, and DeepSeek's
      <a href="https://deepseekcoder.github.io/" target="_blank">DeepSeek Coder</a>, alongside
      various fine-tune variants such as
      <a href="https://github.com/ise-uiuc/magicoder" target="_blank">Magicoder</a> and
      <a href="https://www.phind.com/blog/code-llama-beats-gpt4" target="_blank">Phind-CodeLlama</a
      >.
    </p>
    <p>
      Many of these models are impressive at generating short snippets of code, and often fare well
      even for complex problem descriptions–albeit with some hallucinations due to the probabilistic
      nature of LLMs. In this blog post we explore the following:
      <em
        >is it possible to achieve LLM code generation that is accurate, consistent, and relatively
        cheap to run?</em
      >
    </p>
    <p>
      We approach the problem by combining a pre-trained transformer model with an adapted version
      of the
      <a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search" target="_blank"
        >Monte-Carlo tree search</a
      >
      (MCTS) algorithm to guide the generation, taking inspiration from the paper
      <a href="https://arxiv.org/abs/2303.05510" target="_blank"
        >Planning with Large Language Models for Code Generation</a
      >
      . We compare the effectiveness of this approach against direct sampling of the transformer’s
      unaided generations on the
      <a href="https://github.com/openai/human-eval" target="_blank">HumanEval</a> and
      <a href="https://github.com/NVlabs/verilog-eval" target="_blank">VerilogEval</a> benchmarks,
      and detail the process & observations gathered along the way. Jump ahead to the
      <a href="#discussion">final discussion</a> if you’re curious about the results!
    </p>
    <h2>Background</h2>
    <p>
      At a high level, language models output the tokens (characters) that they've been trained to
      recognise as the most probable ones to come after a given sequence or prompt. It can be seen
      as a kind of <em>statistical predictor</em>, and it turns out that such an approach works
      surprisingly well in generating all kinds of text (like source code).
    </p>
    <p>
      But as impressive as they are, LLMs also come with their own quirks and limitations. One
      well-known issue is the tendency for models to hallucinate and generate responses that are
      either subtly incorrect or outright made up. This is obviously not what we want when
      generating code, where we expect the output to be both functionally correct & consistent over
      time.
    </p>
    <img
      loading="lazy"
      src="public/bad-completion-example.png"
      alt="llm-incorrect-example"
      class="llm-example"
    />
    <img
      loading="lazy"
      src="public/llm-code-execution-example.png"
      alt="llm-code-execution-example"
      class="llm-example"
    />
    <p class="caption">
      Example of incorrect Python code output from DeepSeek Coder 6.7B (running on llama.cpp).
    </p>
    <p>
      A naïve solution to this shortcoming is to run multiple LLM generations for a single prompt
      and simply filter out the incorrect ones that fail compilation or a test bench. This is known
      as
      <b>sampling + filtering</b> (S+F), and we adopted it as the baseline against which to compare
      our later
      <a href="https://en.wikipedia.org/wiki/Tree_(data_structure)" target="_blank">tree-based</a>
      approach.
    </p>

    <h2>Setting up with an LLM</h2>
    <p>
      One of our primary constraints for the experiment was to choose a smaller code LLM that is
      relatively cheap to run. While somewhat arbitrary, we decided to define this as any LLM that
      could run locally on an Apple M2 Pro with 32GB unified memory and 16-core GPU as a reasonably
      specced consumer-grade machine.
    </p>
    <p>
      In selecting the specific language model to use, we looked at several popular open-source LLMs
      ranging from 2.7B~34B parameters in size:
      <a href="https://huggingface.co/microsoft/phi-2" target="_blank">Phi-2</a>,
      <a href="https://huggingface.co/oobabooga/CodeBooga-34B-v0.1" target="_blank"
        >CodeBooga-34B-v0.1</a
      >,
      <a href="https://huggingface.co/ise-uiuc/Magicoder-S-DS-6.7B" target="_blank"
        >Magicoder-S-DS-6.7B</a
      >, and
      <a href="https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct" target="_blank"
        >deepseek-coder-6.7b-instruct</a
      >. Our method of appraisal was largely qualitative, running each LLM through a series of
      prompts either via the HuggingFace
      <a href="https://huggingface.co/docs/transformers/index" target="_blank">transformers</a>
      library (<a href="https://github.com/rmshin/llm-mcts/blob/master/nbs/hf.ipynb" target="_blank"
        >example notebook</a
      >), <a href="https://lmstudio.ai/" target="_blank">LM Studio</a> chat, or
      <a href="https://github.com/ggerganov/llama.cpp" target="_blank">llama.cpp’s</a>
      <a
        href="https://github.com/ggerganov/llama.cpp/tree/master/examples/main#quick-start"
        target="_blank"
        >CLI interface</a
      >.
    </p>
    <p>
      The CodeBooga-34B model proved too large to fit in memory unless heavily quantized to 3 bits
      or below (due to Apple defining a
      <a
        href="https://developer.apple.com/documentation/metal/mtldevice/2369280-recommendedmaxworkingsetsize?language=objc"
        target="_blank"
        >hard limit for GPU memory usage</a
      >
      at roughly ~70% of total memory), which degraded the model quality beyond our acceptable
      limit. On the other hand, with the smaller Phi-2 model we struggled to get it to follow the
      output format instructions specified in <a href="#prompting">our prompts</a>, making it
      difficult to automatically evaluate the model’s code output on benchmark test cases.
    </p>
    <p>
      We ultimately settled on the mixed 5/6-bit
      <a
        href="https://huggingface.co/TheBloke/deepseek-coder-6.7B-instruct-GGUF/blob/main/deepseek-coder-6.7b-instruct.Q5_K_M.gguf"
        target="_blank"
        >Q5_K_M</a
      >
      quantized version of <code class="inline-code">deepseek-coder-6.7B-instruct</code>, due to its
      superior quality & inference speed. Running under
      <a href="https://github.com/ggerganov/llama.cpp" target="_blank">llama.cpp</a>, the model
      could generate an average of ~22.5 tokens/s and we integrated it via the python bindings
      provided by
      <a href="https://llama-cpp-python.readthedocs.io/en/latest/" target="_blank"
        >llama-cpp-python</a
      >.
    </p>
    <pre><code class="language-python">from llama_cpp import Llama

model = Llama(
    model_path="deepseek-coder-6.7b-instruct.Q5_K_M.gguf",
    n_gpu_layers=-1, # run the full model on GPU
    n_ctx=2048,
    n_batch=256,
    n_threads=10,
    logits_all=True, # include token probabilities in output
)</code></pre>

    <h2 id="prompting">Prompting</h2>
    <p>
      In order to better process and extract code from the generations given by
      <code class="inline-code">deepseek-coder-6.7B-instruct</code>, we employed
      <a href="https://www.promptingguide.ai/techniques/fewshot" target="_blank"
        >few-shot prompting</a
      >
      to ensure the model followed a structured format and proper indentation as appropriate. We
      relied on templating from the LLM prompting library
      <a href="https://outlines-dev.github.io/outlines/" target="_blank">outlines</a> to create the
      final prompts.
    </p>
    <pre><code class="language-python">import outlines

@outlines.prompt
def few_shot_prompt(examples, question):
    """
    Please answer the following question following the examples.
    Generate valid python code by indenting 4 spaces always.

    {% for example in examples %}
    Question:
    ```
    {{ example.prompt }}
    ```
    Answer:
    ```
    {{ example.canonical_solution }}
    ```
    {% endfor %}

    Question:
    ```
    {{ question }}
    ```
    Answer:
    ```
    """</code></pre>

    <h2>The HumanEval dataset</h2>
    <p>
      We chose to specifically test the model's Python proficiency via the
      <a href="https://github.com/openai/human-eval" target="_blank">HumanEval</a> dataset which,
      though far from being a perfect measure, is an established rubric in evaluating LLM code
      performance. HumanEval consists of 164 programming tasks in Python of varying difficulty, with
      each task containing at least an ID, prompt, reference solution, and test to appraise model
      output. Here is an example of what a single task in the dataset looks like:
    </p>
    <img class="dataset-example" src="public/humaneval.png" alt="human-eval example" />
    <p class="caption">
      (source
      <a
        href="https://www.researchgate.net/figure/Example-Problem-ID-0-from-HumanEval-dataset_fig1_363267006"
        >ResearchGate</a
      >)
    </p>
    <p>
      The dataset also provides the
      <code class="inline-code">evaluate_functional_correctness</code> script to calculate the
      <a href="https://deepgram.com/learn/humaneval-llm-benchmark#the-passk-metric">pass@k</a>
      metric for the LLM-generated code samples. <b>Pass@k</b> is a probabilistic calculation that
      represents the likelihood that at least one of the top k-generated samples for each HumanEval
      task passes its tests. It can be seen as the overall "pass-rate" of the LLM over the entire
      HumanEval problem set.
    </p>

    <h2>Generating S+F baselines</h2>
    <p>
      With the S+F strategy, we expected a higher number of samples generated per prompt to increase
      the LLM's accuracy. In an ideal world, we would've generated upwards of ~200 samples, but due
      to both hardware & time constraints we limited the generations to 20 samples per task. This
      has the downside of higher variance in the ensuing pass@k calculations, but we did not expect
      it to be to so significant as to distort the experiment results.
    </p>

    <p>
      Below is the code for generating the S+F baselines with
      <a href="https://learnprompting.org/docs/basics/configuration_hyperparameters" target="_blank"
        >hyperparameters</a
      >:
    </p>
    <pre><code class="language-python">from humaneval import get_prompts_with_ids
from human_eval.data import write_jsonl
      
N_SAMPLES = 20
prompts_ids = get_prompts_with_ids()
# prompts are already templated to include few-shot examples
for prompt, task_id in prompts_ids:
    samples = []
    for _ in range(N_SAMPLES):
        # hyperparams: top-3 sampling, 1.0 temp, 256 max tokens
        output = model(
            prompt=prompt, max_tokens=256, temperature=1, top_k=3, stop=["```"]
        )
        res = output["choices"][0]["text"]
        item = dict(task_id=task_id, completion=res)
        samples.append(item)

    # save generated samples in jsonl out file
    write_jsonl("few_shot_baselines_256_top_3.jsonl", samples, append=True)</code></pre>

    <p>and the resulting pass@k metrics:</p>
    <div class="table-wrapper">
      <table>
        <thead>
          <tr>
            <th></th>
            <th>pass@1</th>
            <th>pass@5</th>
            <th>pass@20</th>
            <th>time for 20 samples</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>S+F</td>
            <td>62.38%</td>
            <td>84.60%</td>
            <td>90.74%</td>
            <td>~4h</td>
          </tr>
        </tbody>
      </table>
    </div>
    <h4>Discussion</h4>
    <ul>
      <li>
        As expected, there was an increase in the pass@k percentages going from 1 → 5 → 20 samples.
        What was surprising however was the extent of the jump in pass-rates between even 5 and 20
        samples.
      </li>
      <li>
        This suggests that standalone LLM generation can be improved a lot by sampling many times
        with hyperparameters that enable exploration of diverse solutions by the model (e.g. higher
        top-k sampling, temperature, etc.).
      </li>
      <li>
        With a 20-sample pass-rate &gt;90%, even a relatively small 6.7B parameter model at 5-bit
        quantisation displays impressive performance when fine-tuned for a specific use-case (or
        could be overfitted for benchmarks)!
      </li>
      <li>
        The accuracy improvements of S+F do not come for free, however. Both compute budget &
        processing time grow in proportion to the number of samples; 5 sample time was roughly 1/4
        the time for 20 samples (~1h vs ~4h).
      </li>
    </ul>
    <img src="public/pass-k-baseline.png" class="llm-example" />
    <p class="caption">
      Running the HumanEval-provided pass@k evaluation script on LLM-generated samples.
    </p>

    <h2>Code generation and tree search</h2>
    <p>
      Alternatively, code generation with LLMs may be seen as a search problem, where the goal is to
      find the correct sequence of tokens that passes some end objective such as passing a set of
      test cases. In this approach, the task can be modelled using a
      <a href="https://en.wikipedia.org/wiki/Decision_tree" target="_blank">decision tree</a>. Given
      a starting prompt (the <b>root</b>), we map out all the possible next tokens from that point
      (<b>branches</b> or <b>children</b>), and then recursively expand each of the child branches
      in the same manner until the desired level (the tree <b>depth</b>) is reached.
    </p>
    <input type="range" id="time-step-tree" name="time-step-tree" min="0" />
    <label id="time-step-tree-label" for="time-step-tree">Time step:</label>
    <div id="tree-cy"></div>
    <p class="caption">
      A visualisation of a small decision tree. <span class="red">Red</span> represents the root
      node (i.e. the initial prompt), <span class="blue">blue</span> represents child nodes
      (subsequent tokens). <span class="green">Green</span> represents the final set of all possible
      outcomes. Play around with the time-step slider above to see how the tree grows as it deepens.
    </p>
    <p>
      Finding the solution with this method is simply a matter of selecting the best option from the
      final list of outcomes, but this is only possible when the outcome space is small. As a tree's
      depth and <b>branching factor</b> (number of children at each node) grows, the computation
      needed to map the full tree increases exponentially.
    </p>
    <input type="number" id="tree-branch-factor" name="tree-branch-factor" value="2" />
    <label id="branch-factor-label" for="tree-branch-factor">Branching factor</label>
    <label id="branch-factor-outcomes"># total outcomes:</label>
    <label id="branch-factor-depth">(depth = 5)</label>
    <div id="branching-cy"></div>
    <p class="caption">
      Try changing the branching factor in the visualisation above to see how it affects the number
      of total outcomes to compute.
    </p>
    <p>
      Though difficult to estimate, we'd expect code text generation to have a depth & branching
      factor on the order of hundreds, if not thousands. A typical game of Go, for reference, takes
      an
      <a href="https://homepages.cwi.nl/~aeb/go/misc/gostat.html" target="_blank"
        >average of 211 turns to complete</a
      >
      with a branching factor of ~250. This equates to ~10<sup>500</sup> possible outcomes needed to
      be computed, which is already far more than can be processed in any reasonable length of time.
    </p>
    <p>
      It is thus impractical to apply decision trees to code generation without also adopting some
      kind of strategy to significantly reduce the branching factor & resulting search space. This
      is precisely why we employ MCTS.
    </p>
    <p>
      The principal goal of MCTS is to identify the "winning" branches in a tree without computing
      the entire tree. It achieves this by applying a heuristic (the
      <a
        href="https://www.geeksforgeeks.org/upper-confidence-bound-algorithm-in-reinforcement-learning/"
        target="_blank"
        >upper confidence bound</a
      >) to balance <b>exploiting</b> known good options with <b>exploring</b> unknown, potentially
      better options. This results in an asymmetric expansion of the tree, where only the most
      promising subtrees are computed.
    </p>
    <div id="asymmetric-cy"></div>
    <p class="caption">Example tree with uneven development of branches.</p>

    <h2>Combining MCTS with LLMs</h2>
    <p>
      The way MCTS works is by repeatedly cycling through four logical stages: selection, expansion,
      evaluation (or simulation), and back-propagation.
    </p>
    <img loading="lazy" src="public/mcts-steps.svg" alt="mcts steps" class="mcts-steps" />
    <p class="caption">
      High-level diagram of steps in conventional Monte-Carlo tree search (source
      <a
        href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search#/media/File:MCTS-steps.svg"
        target="_blank"
        >Wikipedia</a
      >).
    </p>
    <p>
      In the <b>selection</b> phase, the MCTS algorithm makes a decision on which node or branch it
      should explore next via the algorithm's <b>rollout policy</b>. This is a function that assigns
      a kind of "priority" to each node, with greater weight given to nodes that are either higher
      in value or have fewer visits. For our case of LLM code generation, the exact rollout policy
      is calculated by the following:
    </p>
    <img loading="lazy" src="public/p-ucb.png" alt="p-ucb function" class="p-ucb" />
    <p>and the final node selection:</p>
    <img loading="lazy" src="public/p-ucb-select.png" alt="p-ucb selection" class="p-ucb-select" />
    <p>
      It is not necessary to understand the above in detail. What's more important is to realise
      that the choice of which nodes to explore is a function of three main parameters: the node
      value
      <b>Q</b> (to be elaborated further below), the node token probability <b>P</b> (provided by
      the LLM), and an <b>exploration term</b> that captures how often we've previously visited the
      node.
    </p>
    <p>
      Once a node has been selected, it is then expanded with <em>k</em> new child nodes in the
      <b>expansion</b> phase. Because only a single branch is selected at any given point in time,
      MCTS develops the search tree asymmetrically by disregarding all other nodes. In this
      experiment, the child nodes are obtained from the top 3 most probable next token options given
      by an LLM.
    </p>
    <p>
      Following expansion comes the <b>evaluation</b> step. It is here that the value of the node is
      determined via the <b>reward</b> or <b>value function</b>–a quantitative measure to rank
      nodes, such that choosing higher value nodes would yield better end outcomes. But each node in
      the tree represents an incomplete intermediate state that is often difficult to evaluate
      directly. Thus we instead evalute a chosen node's eventual end outcome by
      <b>simulating</b> the remaining branch expansions.
    </p>
    <p>
      We achieve this simulation by having an LLM complete the remaining sequence (program) from the
      node's partial state. This completed program is then evaluated against a bench of tests from
      the HumanEval (or VerilogEval) datasets, to obtain a pass rate between 0 - 1 that is assigned
      as the node's value. In this manner, code that passes more tests are explored further in
      future iterations through the tree.
    </p>
    <p>
      But due to the nested nature of search trees, any high-value nodes discovered deep in the
      hierarchy might not be reached again in successive searches (e.g. if the previous parent
      branches have low values). To ensure that this information isn't "lost", a node's reward value
      is propagated to all it's parents up to the tree's root. This is the final
      <b>back-propagation</b> phase that completes a single iteration of MCTS, and once finished we
      return to the top (root) of the tree to restart the cycle.
    </p>

    <h2>Coding the MCTS algorithm</h2>
    <p>
      We use a simple Node class to represent the nodes of decision tree, which has attributes liek
      state, parent, children, visits, and rewards and p_ucb metric.
    </p>
    <pre><code class="language-python">class Node:
    def __init__(self, prob, state, parent):
        self.value = 0  # total reward obtainable from node
        self.prob = prob  # necessary for P-UCB calculation
        self.state = state  # full generated text
        self._children = []
        self._parent = parent
        self.visits = 0

    def backprop(self, value):
        # only propagate if new reward is greater than current max
        if value > self.value:
            self.value = value
            if self._parent is not None:
                self._parent.backprop(value)</code></pre>
    <p class="caption">
      Remember that a node is equivalent to a branch in the Monte-Carlo tree, and it represents a
      possible token (character) that could come next in the sequence of generated text.
    </p>
    <pre><code class="language-python">max_rollouts = 128 # max number of iterations through tree
top_k = 3 # number of child tokens (branches) to generate per node
for prompt, task_id in prompts:
    # cache of generated programs => rewards
    program_dict = {}
    root = Node(prob=1, state=prompt, parent=None)</code></pre>
    <p>
      The tree is expanded and evaluated for a fixed number of iterations called rollout. In each
      rollout, we peform selection, expansion, evaluation and backprop. At the end, we return the
      best node.
    </p>
    <pre><code class="language-python">    for i in range(max_rollouts):
        curr_node = root
        curr_node.visits += 1</code></pre>
    <p>According to a P-UCB metric, we select the best leaf child of the current node to expand.</p>
    <pre><code class="language-python">        # selection
        while len(curr_node._children) > 0:
            curr_node = p_ucb_select(curr_node._children)
            curr_node.visits += 1</code></pre>
    <p>We then expand the selected node by generating top-k tokens and creating child nodes.</p>
    <pre><code class="language-python">        # expansion
        tokens = get_top_k_tokens(curr_node, top_k)
        child_nodes = [
            Node(prob, state=(curr_node.state + token), parent=curr_node)
            for (token, prob) in tokens
        ]
        curr_node._children = child_nodes</code></pre>
    <p>
      We then evaluate the child nodes, by generating a program from the current state. This is
      usually done using a greedy search, but can easily be replaced with a beam search or other
      search algorithms. We believe that Code Gen should be done using a greedy search as opposed to
      sampling but it should be tested. The generated programs are evaluated by running tests on
      them. Look below for the calculate_reward function.
    </p>
    <pre><code class="language-python">        # evaluation
        reward = match_cached_programs(curr_node.state, program_dict)
        # only run generation if node state not found in cached programs
        if reward == -1:
            generated_program = llm_generate(curr_node.state)
            # run generated program against HumanEval test cases
            reward = calculate_reward(prompt, generated_program)
            program_dict[generated_program] = reward
          </code></pre>
    <p>
      Finally we backprop the reward up the tree to all the nodes upto the root. We stop when a
      reward of 1 is found, as there is no more exploration to do.
    </p>
    <pre><code class="language-python">    # backprop reward up the tree
    curr_node.backprop(reward)

    # early termination if correct program is found
    if reward == 1:
        break</code></pre>

    <h4>Node selection:</h4>
    <pre><code class="language-python">def p_ucb_select(parent_node, child_nodes):
    s_visits = parent_node.visits
    # scalar constant term
    beta = log((s_visits + c_base + 1) / c_base) + c

    # find the child node with the highest P-UCB value
    max_p_ucb = -inf
    max_node = None
    for i in range(len(child_nodes)):
        node = child_nodes[i]
        p_ucb = node.value + beta * node.prob * sqrt(log(s_visits)) / (
            1 + node.visits
        ) # calculate the P-UCB value for each child
        if p_ucb > max_p_ucb:
            max_node = node
            max_p_ucb = p_ucb
    return max_node # return max(P-UCB) node</code></pre>
    <h4>Rollout policy:</h4>
    <pre><code class="language-python"># fetch the top 3 highest probability token candidates from the LLM
def get_top_k_tokens(curr_node, k=3):
    output = model(prompt=curr_node.state, max_tokens=1, temperature=1, logprobs=k)
    output_probs = output["choices"][0]["logprobs"]["top_logprobs"][0]
    return output_probs.items()</code></pre>
    <h4>Reward function:</h4>
    <pre><code class="language-python">def calculate_reward(task_id, completion, timeout=10):
    problem = human_eval_problems[task_id]
    split_tests = problem["test"]
    results = []
    for test in split_tests:
        res = check_correctness(test, completion, timeout)
        results.append(res["passed"])

    return sum(results) / len(results) # set test pass-rate as reward</code></pre>

    <h2>Visualizing the MCTS Tree</h2>
    <p>
      Here we visulize searching for a solution to HumanEval-113 using MCTS. The tree is visualized
      at each time step, and the nodes are colored red if they are selected for expansion. Finally
      on rollout no. x, we find a solution.
    </p>
    <input type="range" id="time-step-rollout" name="time-step-rollout" min="0" />
    <label id="time-step-rollout-label" for="time-step-rollout">Time step:</label>
    <div id="rollout-cy"></div>

    <h2>Beating the baseline on HumanEval</h2>
    <p>With hyper parameters: 2048 context size, top-3 sampling, 1.0 temp, 256 max tokens</p>
    <div class="table-wrapper">
      <table>
        <thead>
          <tr>
            <th></th>
            <th>pass@5</th>
            <th>pass@20</th>
            <th>time for 5 samples</th>
            <th>time for 20 samples</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>S + F</td>
            <td>84.60%</td>
            <td>90.74%</td>
            <td>~1h</td>
            <td>~4h</td>
          </tr>
        </tbody>
      </table>
    </div>
    <p>For MCTS with 128 max rollouts, 2048 context, top-3 probs, 1.0 temp, 256 max tokens</p>
    <div class="table-wrapper">
      <table>
        <thead>
          <tr>
            <th></th>
            <th>pass rate</th>
            <th>avg unique generations</th>
            <th>avg rollouts</th>
            <th>time</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>MCTS</td>
            <td>92.59%</td>
            <td>3.62</td>
            <td>15.46</td>
            <td>~1h 30m</td>
          </tr>
        </tbody>
      </table>
    </div>

    <h4>Discussions</h4>
    <ul>
      <li>
        <b>pass@k:</b> Big jump in pass rate from 5 → 20 samples suggest basic LLM generation can be
        improved by simply tuning generation parameters (e.g. top-k sampling, temperature, higher
        max tokens, etc.), and exploring many diverse solutions.
      </li>
      <li>
        Small fine-tuned models of around 6.7b params and 5-bit quantisation still display
        impressive performance (or could be overfitted for benchmarks)!
      </li>
      <li>
        MCTS achieves better benchmark performance than 20-sample baseline with an average of less
        than 4 samples per problem (i.e. &lt;1/5 transformer compute<sup>
          <a href="#callout">[1]</a></sup
        >). The compute is saved because of caching of previously generated programs, rewards and
        token probabilities.
      </li>
      <li>
        MCTS has explored almost 15.46 solutions (avg rollouts) per problem about 2x faster, to
        achieve a higher accuracy than S+F 20. It is to be noted, that generations in MCTS undergo
        tests which will lead to overhead.
      </li>
    </ul>

    <ol id="callout" class="caption">
      <li>
        One may suggest measuring the avg roll outs until sucess in S+F, but because of the
        probablistic nature, which cannot guarantee replication, we rather report the expected
        metrics like pass@k from some finite samples.
      </li>
    </ol>
    <h2>Extending the experiment to VerilogEval</h2>
    <p>
      What happens when dealing with problems that the LLM is not well trained on? How effective is
      MCTS at improving LLM code quality?
    </p>
    <img class="dataset-example" src="public/verilogeval.png" alt="verlilog-eval example" />
    <p class="caption">
      (source
      <a href="https://arxiv.org/abs/2309.07544">VerilogEval</a>)
    </p>
    <p>With hyper parameters: 4096 context size, top-50 sampling, 1.0 temp, 1024 max tokens</p>
    <div class="table-wrapper">
      <table>
        <thead>
          <tr>
            <th></th>
            <th>pass@5</th>
            <th>pass@10</th>
            <th>pass@20</th>
            <th>time for 20 samples</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>S + F</td>
            <td>42.58%</td>
            <td>47.33%</td>
            <td>51.30%</td>
            <td>~7h</td>
          </tr>
        </tbody>
      </table>
    </div>
    <p>
      For MCTS, with hyperparameters: 128 max rollouts, 4096 context, top-5 probs, 1.0 temp, 1024
      max tokens
    </p>
    <div class="table-wrapper">
      <table>
        <thead>
          <tr>
            <th></th>
            <th>pass rate</th>
            <th>avg unique generations</th>
            <th>avg rollouts</th>
            <th>time</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>MCTS</td>
            <td>46.75%</td>
            <td>18.18</td>
            <td>73.46</td>
            <td>~8h 30m</td>
          </tr>
        </tbody>
      </table>
    </div>
    <h4>Discussion</h4>
    <ul>
      <li>
        We had to increase the context length & max number of generated tokens to account for
        greater verbosity of verilog code.
      </li>
      <li>
        As this deepseek model is not trained on verilog data, MCTS required significantly more
        exploration (greater rollouts) despite achieving lower quality/pass-rate than compared to
        its performance on HumanEval.
      </li>
      <li>
        S+F fared better than MCTS with a 10 and 20-sample baseline and was more time efficient than
        MCTS. The mean test time for verilog was 0.71s when compared to python which was 3.82s,
        highlighting the fact that test time did not account for much.
      </li>
      <li>
        MCTS is probably failing to explore solutions even in a structured manner because the
        transfomer’s token probabilities are badly calibrated and less reliable. This is just due
        tot the fact that it has not seen enough verilog data. However, it has probably learnt good
        programming principles to solve at least 45%.
      </li>
      <li>
        S+F introduces more randomness/variability in generation and therefore explores wider and
        fares better (as it samples from top-50 token probabilities).
      </li>
    </ul>

    <h4>How to make MCTS better?</h4>
    <ol>
      <li>
        <b>Reward:</b> A better heuristic to determine the validity of a partial program, could
        guide the search better.
      </li>
      <li>
        <b>Policy:</b> As the transformer for verilog code gen was mis-calibrated, we can lower the
        weight given to the transformer probability for it to instead explore more solutions.
      </li>
    </ol>
    <h2 id="discussion">Recap: what have we learned?</h2>
    <p>
      The biggest takeaway from our experiments is that using LLMs with MCTS to generate code is,
      despite some positive results, unlikely to generalise well to a broader range of programming
      tasks and languages without significant rethinking.
    </p>
    <p>
      While combining the LLM's predictive capabilities with a specialised algorithm such as MCTS
      did indeed produce better results in the HumanEval benchmarks than standalone LLM generation,
      this was largely due to our chosen fine-tuned model
      <code class="inline-code">deepseek-coder-6.7B-instruct</code> already having strong baseline
      performance in the Python programming language.
    </p>
    <p>
      Once we transitioned to the VerilogEval benchmarks where the LLM was much weaker, MCTS didn't
      do so well due to its reliance on the LLM’s (usually incorrect) token probabilities to
      determine the next child branches to explore. Direct sampling + filtering of the LLM
      generations performed better in this scenario, particularly as we increased the number of
      samples per problem, but the overall pass rates still remained low for both approaches.
    </p>
    <p>
      This highlights the relative effectiveness of a simple approach such as sampling + filtering
      in boosting LLM performance (albeit at the cost of higher compute), and perhaps more
      importantly the impressive quality obtainable by fine-tuning smaller language models for
      specific tasks such as code generation.
    </p>
    <p>
      But we are nevertheless cautiously optimistic about the general approach of interweaving LLM
      capabilities with traditional/alternative algorithms (or even other LLMs) for greater
      robustness & accuracy. Recent research in
      <a
        href="https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/"
        target="_blank"
        >using LLMs with evolutionary algorithms</a
      >, <a href="https://arxiv.org/abs/2401.10020" target="_blank">self-rewarding LLMs</a>, and
      other <a href="https://arxiv.org/abs/2401.08500" target="_blank">systemic strategies</a> show
      promise in this direction, and we expect to develop on both our experiment & these ideas for
      future enquiry.
    </p>
  </body>
  <script>
    var treeData;
    const treeGraph = document.getElementById('tree-cy');
    function renderTree(timeStep) {
      const tree = treeData[timeStep];
      const treeCy = cytoscape({
        container: treeGraph,
        elements: {
          nodes: tree.nodes,
          edges: tree.edges,
        },
        panningEnabled: false,
        boxSelectionEnabled: false,
        layout: {
          name: 'dagre',
          align: 'UL',
        },
        style: [
          {
            selector: 'node',
            style: {
              'background-color': function (el) {
                return Number(el.id()) > 14 ? 'green' : '#11479e';
              },
            },
          },
          {
            selector: 'edge',
            style: {
              width: 3,
              'line-color': '#9dbaea',
              'target-arrow-color': '#9dbaea',
              'target-arrow-shape': 'triangle',
            },
          },
        ],
      });
      treeCy.autolock(true);
    }
    fetch('public/graph_example_tree.json')
      .then((res) => res.json())
      .then((data) => {
        treeData = data;
        const treeSlider = document.getElementById('time-step-tree');
        const treeSliderLabel = document.getElementById('time-step-tree-label');
        treeSlider.max = Math.max.apply(null, Object.keys(treeData));

        // handle first render
        renderTree(treeSlider.value);
        treeSliderLabel.textContent = `Time step: ${treeSlider.value}`;

        // update tree upon time-step input changes
        treeSlider.addEventListener(
          'input',
          (e) => (treeSliderLabel.textContent = `Time step: ${treeSlider.value}`)
        );
        treeSlider.addEventListener('input', (e) => renderTree(e.target.value));
      });
  </script>
  <script>
    var branchData;
    const branchGraph = document.getElementById('branching-cy');
    function renderBranching(factor) {
      const tree = branchData[factor];
      const treeCy = cytoscape({
        container: branchGraph,
        elements: {
          nodes: tree.nodes,
          edges: tree.edges,
        },
        panningEnabled: false,
        boxSelectionEnabled: false,
        layout: {
          name: 'dagre',
        },
        style: [
          {
            selector: 'node',
            style: { 'background-color': '#11479e' },
          },
          {
            selector: 'edge',
            style: {
              width: 3,
              'line-color': '#9dbaea',
              'target-arrow-color': '#9dbaea',
              'target-arrow-shape': 'triangle',
            },
          },
        ],
      });
      treeCy.autolock(true);
    }
    fetch('public/graph_branching_factor.json')
      .then((res) => res.json())
      .then((data) => {
        branchData = data;
        const bfInput = document.getElementById('tree-branch-factor');
        const bfOutcomesLabel = document.getElementById('branch-factor-outcomes');
        bfInput.min = Math.min.apply(null, Object.keys(branchData));
        bfInput.max = Math.max.apply(null, Object.keys(branchData));
        const depth = 4; // hardcoded depth of tree in example json

        // handle first render
        renderBranching(bfInput.value);
        bfOutcomesLabel.textContent = `# total outcomes: ${bfInput.value ** depth}`;

        // update tree upon branching factor input changes
        bfInput.addEventListener('input', (e) => renderBranching(e.target.value));
        bfInput.addEventListener('input', (e) => {
          bfOutcomesLabel.textContent = `# total outcomes: ${e.target.value ** depth}`;
        });
        bfInput.addEventListener('blur', (_) => {
          const val = Math.max(bfInput.min, Math.min(bfInput.value, bfInput.max));
          if (bfInput.value != val) renderBranching(val);
          bfInput.value = val;
        });
      });
  </script>
  <script>
    const asymGraph = document.getElementById('asymmetric-cy');
    fetch('public/graph_asymmetric.json')
      .then((res) => res.json())
      .then((data) => {
        const treeCy = cytoscape({
          container: asymGraph,
          elements: {
            nodes: data.nodes,
            edges: data.edges,
          },
          panningEnabled: false,
          boxSelectionEnabled: false,
          layout: {
            name: 'dagre',
          },
          style: [
            {
              selector: 'node',
              style: { 'background-color': '#11479e' },
            },
            {
              selector: 'edge',
              style: {
                width: 3,
                'line-color': '#9dbaea',
                'target-arrow-color': '#9dbaea',
                'target-arrow-shape': 'triangle',
              },
            },
          ],
        });
        treeCy.autolock(true);
      });
  </script>
  <script>
    var graphData;
    const rolloutGraph = document.getElementById('rollout-cy');
    function renderGraph(timeStep) {
      const graph = graphData[timeStep];
      const rolloutCy = cytoscape({
        container: rolloutGraph,
        elements: {
          nodes: graph.nodes.map((node) => ({ data: { ...node } })),
          edges: graph.edges.map(([n1, n2]) => ({
            data: { source: n1, target: n2, selectable: false },
          })),
        },
        style: [
          {
            selector: 'node',
            style: {
              'background-color': function (el) {
                return graph.selectedNodes.includes(Number(el.id())) ? 'red' : '#666';
              },
              label: 'data(label)',
            },
          },
          {
            selector: 'edge',
            style: {
              width: 3,
              'line-color': function (el) {
                return graph.selectedNodes.includes(Number(el.data('target'))) ? 'red' : '#ccc';
              },
              'target-arrow-color': function (el) {
                return graph.selectedNodes.includes(Number(el.data('target'))) ? 'red' : '#ccc';
              },
              'target-arrow-shape': 'triangle',
              'curve-style': 'bezier',
            },
          },
        ],
        panningEnabled: false,
        boxSelectionEnabled: false,
        layout: {
          name: 'dagre',
          align: 'UL',
        },
      });
      rolloutCy.autolock(true);
    }
    fetch('public/graph_HumanEval_113.json')
      .then((res) => res.json())
      .then((data) => {
        graphData = data;
        const rolloutSlider = document.getElementById('time-step-rollout');
        const rolloutSliderLabel = document.getElementById('time-step-rollout-label');
        rolloutSlider.max = Math.max.apply(null, Object.keys(graphData));

        // handle first render
        renderGraph(rolloutSlider.value);
        rolloutSliderLabel.textContent = `Time step: ${rolloutSlider.value}`;

        // update tree upon time-step input changes
        rolloutSlider.addEventListener(
          'input',
          (e) => (rolloutSliderLabel.textContent = `Time step: ${rolloutSlider.value}`)
        );
        rolloutSlider.addEventListener('input', (e) => renderGraph(e.target.value));
      });
  </script>
</html>
